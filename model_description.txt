The model is basically a gradient boosting decision tree model.
The features come from three parts:
(1) Baseline features, which come from fake news challenge baseline, 44 dimensions
(2) Word2vec features, which come from Google news word2vec data, and each text is represented by the mean pooling of all its words (stop words removed), 300 dimensions
(3) LDA features, which come from LDA model trained by all the corpus, which has 25 topics, and one extra cos-similarity between headline features and body features, 51 dimensions.

The features then input into a LightGBM (light gradient boosting model, a fast gbdt) model and trained with 400 iterations and early stop. Light GBM is (1) Fast, which is easier for repeatly re-train, thus good for model updating, (2) Combine features together, which is meaningful in LDA features (final score 1.3% better than xgboost), (3) Will calculate score for each class, good for giving better prediction results.

The model finally gives 80.7% score of total score. If use only one of the three feature sets, the accuracy are around 75.2%, 76.7%, 74.3%, respectively. 

